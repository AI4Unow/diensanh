"""
FastAPI backend for Di√™n Sanh chatbot.
Provides RAG-powered chat endpoint using api.ai4u.now.
"""

import os
import sys
from pathlib import Path

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from openai import OpenAI
from pydantic import BaseModel

# Determine base directory
BASE_DIR = Path(__file__).parent.parent.parent if '__file__' in dir() else Path.cwd()

# Add src to path for imports
sys.path.insert(0, str(BASE_DIR / "src"))

from config import settings

# Import vector store (will be loaded lazily)
vector_store = None

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI(
    title="Di√™n Sanh Chatbot API",
    description="RAG-powered chatbot for Di√™n Sanh commune public services",
    version="1.0.0"
)

# CORS configuration for widget embedding
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def get_vector_store():
    """Lazy-load vector store."""
    global vector_store
    if vector_store is None:
        # Import from parent directory
        import importlib.util
        vs_path = BASE_DIR / "src" / "vector-store.py"
        spec = importlib.util.spec_from_file_location("vector_store", vs_path)
        vs_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(vs_module)
        vector_store = vs_module.VectorStore(persist_dir=str(BASE_DIR / "data" / "vector_store"))
    return vector_store


def get_llm_client() -> OpenAI:
    """Get OpenAI-compatible client for api.ai4u.now."""
    api_key = settings.ai4u_api_key or os.getenv("AI4U_API_KEY")
    if not api_key:
        raise HTTPException(
            status_code=500,
            detail="AI4U API key not configured. Set AI4U_API_KEY environment variable."
        )

    return OpenAI(
        base_url=settings.ai4u_base_url,
        api_key=api_key
    )


# Request/Response models
class ChatRequest(BaseModel):
    message: str
    conversation_id: str | None = None
    include_sources: bool = True


class ChatResponse(BaseModel):
    response: str
    sources: list[dict] | None = None
    conversation_id: str | None = None


class HealthResponse(BaseModel):
    status: str
    documents_indexed: int
    model: str


# System prompt for the chatbot
SYSTEM_PROMPT = """B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa UBND x√£ Di√™n Sanh, t·ªânh Qu·∫£ng Tr·ªã.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† h·ªó tr·ª£ ng∆∞·ªùi d√¢n t√¨m hi·ªÉu th√¥ng tin v·ªÅ:
- C√°c th·ªß t·ª•c h√†nh ch√≠nh c√¥ng (ƒëƒÉng k√Ω khai sinh, k·∫øt h√¥n, c·∫•p gi·∫•y t·ªù, v.v.)
- Th√¥ng tin v·ªÅ UBND x√£ v√† c√°c c∆° quan li√™n quan
- H∆∞·ªõng d·∫´n quy tr√¨nh, h·ªì s∆° c·∫ßn thi·∫øt, th·ªùi gian x·ª≠ l√Ω, ph√≠/l·ªá ph√≠

Quy t·∫Øc tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu
2. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ng·ªØ c·∫£nh
3. N·∫øu kh√¥ng c√≥ th√¥ng tin, n√≥i r√µ v√† h∆∞·ªõng d·∫´n li√™n h·ªá UBND x√£
4. Cung c·∫•p th√¥ng tin li√™n h·ªá khi c·∫ßn: ƒêi·ªán tho·∫°i, ƒë·ªãa ch·ªâ, email (n·∫øu c√≥ trong ng·ªØ c·∫£nh)
5. N·∫øu th·ªß t·ª•c c√≥ b∆∞·ªõc th·ª±c hi·ªán, li·ªát k√™ r√µ r√†ng t·ª´ng b∆∞·ªõc

Lu√¥n th√¢n thi·ªán v√† s·∫µn s√†ng h·ªó tr·ª£ ng∆∞·ªùi d√¢n."""


def build_context(query: str, n_results: int = 5) -> str:
    """Retrieve relevant context from vector store."""
    try:
        store = get_vector_store()
        results = store.search(query, n_results=n_results)

        if not results:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu."

        context_parts = []
        for i, r in enumerate(results, 1):
            title = r["metadata"].get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
            source = r["metadata"].get("source", "unknown")
            content = r["content"][:1500]  # Limit content length

            context_parts.append(f"[{i}] {title}\n(Ngu·ªìn: {source})\n{content}")

        return "\n\n---\n\n".join(context_parts)

    except Exception as e:
        print(f"Error retrieving context: {e}")
        return ""


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    try:
        store = get_vector_store()
        doc_count = store.count()
    except:
        doc_count = 0

    return HealthResponse(
        status="healthy",
        documents_indexed=doc_count,
        model=settings.chat_model
    )


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chat endpoint.
    Uses RAG to retrieve relevant context and generate response.
    """
    if not request.message.strip():
        raise HTTPException(status_code=400, detail="Message cannot be empty")

    # Retrieve relevant context
    context = build_context(request.message)

    # Build messages for LLM
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {
            "role": "user",
            "content": f"""Ng·ªØ c·∫£nh (th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu):
---
{context}
---

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√¢n: {request.message}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh tr√™n."""
        }
    ]

    # Call LLM
    try:
        client = get_llm_client()
        response = client.chat.completions.create(
            model=settings.chat_model,
            messages=messages,
            temperature=0.3,  # Lower for more factual responses
            max_tokens=1024
        )

        answer = response.choices[0].message.content

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM error: {str(e)}")

    # Prepare sources if requested
    sources = None
    if request.include_sources:
        try:
            store = get_vector_store()
            results = store.search(request.message, n_results=3)
            sources = [
                {
                    "title": r["metadata"].get("title", ""),
                    "url": r["metadata"].get("url", ""),
                    "score": r["score"]
                }
                for r in results
            ]
        except:
            sources = []

    return ChatResponse(
        response=answer,
        sources=sources,
        conversation_id=request.conversation_id
    )


@app.get("/", response_class=HTMLResponse)
async def root():
    """Simple test page."""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Di√™n Sanh Chatbot API</title>
        <style>
            body { font-family: system-ui; max-width: 600px; margin: 50px auto; padding: 20px; }
            h1 { color: #1a5f2a; }
            .status { background: #e8f5e9; padding: 15px; border-radius: 8px; }
        </style>
    </head>
    <body>
        <h1>üèõÔ∏è Tr·ª£ l√Ω ·∫£o UBND x√£ Di√™n Sanh</h1>
        <div class="status">
            <p>‚úÖ API ƒëang ho·∫°t ƒë·ªông</p>
            <p>üìñ Xem t√†i li·ªáu API: <a href="/docs">/docs</a></p>
            <p>üîß Health check: <a href="/health">/health</a></p>
        </div>
    </body>
    </html>
    """


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "api-server:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=True
    )
